========================================
MetaMorph Beta Release (0.1b)
========================================
This is the first public release of our library framework for interoperable kernels on multi- and many-core Clusters, namely MetaMorph. Since this release is a work-in-progress academic prototype, and it is not production-ready, you are likely to face issues. MetaMorph will incorporate additional HPC kernels (e.g. sorting, dynamic programming, n-body) and run-time services developed by the Synergy Laboratory @ Virginia Tech (http://synergy.cs.vt.edu/). Email the authors for more details. 

MetaMorph is created as part of the Air Force Office of Scientific Research (AFOSR) Computational Mathematics Program via Grant number FA9550-12-1-0442. 

OpenCL code is largely generated by CU2CL. CU2CL has been supported in part by NSF I/UCRC IIP-0804155 via the NSF Center for High-Performance Reconfigurable Computing.

Authors:
	Paul Sathre and Ahmed Helal (Design and implementation)
	Sriram Chivukula (CUDA dot-product/reduce prototypes)
	Kaixi Hou (CUDA data marshaling prototypes)
	Anshuman Verma (FPGA back-end)
	
Contact Email: 
	{ammhelal, sath6220}_at_vt.edu	

Virginia Polytechnic Institute and State University, 2013-2016.

----------------------------------------
News & Updates
----------------------------------------
Nov 12, 2016: Version 0.1b is available.

----------------------------------------
Publications
----------------------------------------
“MetaMorph: A Library Framework for Interoperable Kernels on Multi- and Many-core Clusters.“ Ahmed E. Helal, Paul Sathre, Wu-chun Feng. In Proceedings of the IEEE/ACM International Conference for High Performance Computing, Networking, Storage and Analysis (SC|16), Salt Lake City, Utah, USA, November 2016.

“MetaMorph: A Modular Library for Democratizing the Acceleration of Parallel Computing across Heterogeneous Devices.” Paul Sathre, Wu-chun Feng. In ACM/IEEE International Conference on High-Performance Computing, Networking, Storage, and Analysis (SC|14), New Orleans, LA, USA, November 2014 (poster).
	
----------------------------------------
MetaMorph Overview
----------------------------------------
MetaMorph is designed to effectively utilize HPC systems that consist of multiple heterogeneous nodes with different hardware accelerators. It acts as middleware between the application code and compute devices, such as CPUs, GPUs, Intel MIC and FPGAs. MetaMorph hides the complexity of developing code for and executing on heterogeneous platform by acting as a unified “meta-platform.” The application developer needs only to call MetaMorph’s computation and communication APIs, and the operations are transparently mapped to the proper compute devices. MetaMorph uses a modular layered design, where each layer supports one of its core design principles and each module can be used relatively independently.

----------------------------------------
MetaMorph Design
----------------------------------------
MetaMorph provides programmability, functional portability, and performance portability by abstracting software backends (currently, OpenMP, CUDA and OpenCL) behind a single interface. It achieves high performance by providing low-level implementations of common operations, based on the best-known solutions for a given compute platform. Moreover, the software back-ends are instantiated and individually tuned for the different heterogeneous and parallel computing platforms (currently, multicore CPUs, Intel MICs, AMD GPUs, and NVIDIA GPUs). 

MetaMorph provides an infrastructure for adding software back-ends for future compute devices — without end-user intervention or modifying the application. This provides the small population of early-adopter, architecture experts with a framework that enables them to dramatically extend the impact of their expertise to the wider community by expanding the library with new design patterns. So, rather than writing a kernel once for a single application, these experts can write that same kernel within the MetaMorph framework, provide it to the community, and allow it to be used across many applications. In addition, MetaMorph accelerates the development of new operations and computation/communication patterns by providing a compilation infrastructure and helper APIs that handle the boilerplate initialization and compilation and simplify data exchange between the host and accelerators, such that MetaMorph developers can focus on developing the new kernels.

Existing kernels (e.g., CUDA kernels) can be included in MetaMorph without re-factoring by adding their implementation directly into the interoperability layer (e.g., CUDA backend) and their C/Fortran interface in the abstraction layer. However, advanced features, like seamless execution on different accelerators within a node and across nodes, will work only if these kernels are implemented in all the different back-ends. Contribution of a kernel for even a single back-end is valuable as it provides the architecture-expert community a baseline from which to implement and integrate kernels for the remaining back-ends.

----------------------------------------
MetaMorph Implementation
----------------------------------------
MetaMorph implements the offload/accelerator computation model, in which data is explicitly allocated, copied, and manipulated via kernels within the MetaMorph context. We realize it as a layered library of libraries. The top-level user APIs and platform-specific back-ends exists as separate shared library objects, with interfaces designated in shared header files. The top-level API improves the programmability of user applications by abstracting the backends,which provide accelerated kernels for each platform. It intercepts calls to the MetaMorph communication and computation kernels and transparently maps them to a back-end accelerator supported by the underlying platform. Back-ends are responsible for providing a standard C interface to the accelerated kernels. They are segregated from one another in order to allow separate compilation and encapsulation of platform-specific nuances. Consequently, if a given back-end requires special-purpose libraries or tools to build that are not present on the target machine, it can be easily excluded from a given build of the library as a whole without loss of function in the remaining back-ends.

----------------------------------------
Dependencies
----------------------------------------
	make
	GNU C Library (glibc)
	
	Communication Interface
		MPI (tested with MPICH 3.1.4, MPICH 3.2 and OpenMPI 1.6.4)
	
	OpenMP-backend
		GNU GCC compiler (tested with gcc 4.5, 4.7.2, 4.8.2 and 4.9.2)
		Intel C/C++ Compiler (tested with icc 13.1)
		
	CUDA-backend
		NVIDIA CUDA toolkit (tested with nvcc 5, 6 and 7.5)
		
	OpenCL-backend
		GNU GCC compiler (tested with gcc 4.5, 4.7.2, 4.8.2 and 4.9.2)
		OpenCL libs

----------------------------------------
Usage
----------------------------------------
* Set the root directories (MPICH_DIR and MM_DIR) in the top-level Makefile.
* To build the library with all the supported backends: $make metamorph_all.  
* You may build the library with one or more backends using the additional make targets.
* To build the examples: $make examples. 
* Include metamorph/lib into the LD_LIBRARY_PATH environment variable.
* Change the working directory to metamorph/examples and run the executables using the app-specific options. 

----------------------------------------
License 
----------------------------------------
Please refer to the included LICENSE file.

