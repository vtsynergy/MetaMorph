/** \file
 * The core library header for the METAMORPH Accelerated CFD Library
 * (C) Virginia Polytechnic Institute and State University, 2013-2020.
 *  See attached LICENSE terms before continued use of the code.
 *
 * Created as part of the Air Force Office of Scientific Research
 *  (AFOSR) Computational Mathematics Program via Grant number
 *  FA9550-12-1-0442.
 *
 * OpenCL code is largely generated by CU2CL. CU2CL has been supported in part
 *  by NSF I/UCRC IIP-0804155 via the NSF Center for High-Performance
 *  Reconfigurable Computing.
 *
 * Features to support MetaCL and dynamically-loaded refactoring to support
 *  precompiled binary distribution supported in part by NSF I/UCRC CNS-1266245
 *  via the NSF Center for High-Performance Reconfigurable Computing and by NSF
 *  I/UCRC CNS-1822080 via the NSF Center for Space, High-Performance, and
 *  Resilient Computing.
 *
 * Authors:
 *  Paul Sathre and Ahmed Helal (Original Design and implementation)
 *  Paul Sathre (Dynamically-loaded refactoring -- v0.3b)
 *  Paul Sathre (MetaCL support features -- v0.2b)
 *  Sriram Chivukula (CUDA dot-product/reduce prototypes)
 *  Kaixi Hou (CUDA data marshaling prototypes)
 *  Anshuman Verma (FPGA back-end)
 */

#include <metamorph_emulatable.h>

/** The top-level user APIs **/
#ifndef METAMORPH_H
#define METAMORPH_H
#ifdef __cplusplus
extern "C" {
#endif

#include <stdio.h>

//#define ALIGNED_MEMORY
//#define ALIGNED_MEMORY_PAGE	64

// This needs to be here so that the back-ends can internally
// switch between implementations for different primitive types
typedef enum {
  meta_db = 0, // double
  meta_fl = 1, // float
  meta_lo = 2, // long
  meta_ul = 3, // unsigned long
  meta_in = 4, // int
  meta_ui = 5, // unsigned int
  meta_sh = 6, // short
  meta_us = 7, // unsigned short
  meta_ch = 8, // char
  meta_uc = 9  // unsigned char
} meta_type_id;
size_t get_atype_size(meta_type_id type);

/** Define limits to the meta_face structure */
#define METAMORPH_FACE_MAX_DEPTH (10)

/// \todo FIXME Transpose shouldn't use hardcoded constants
/// \todo FIXME even if it does, OpenCL will need to pass them during compilation
/** Primary transpose tile dimension */
#define TRANSPOSE_TILE_DIM (16)
/** Largely deprecated, for non-square transpose tiles, define the number of
 * rows */
#define TRANSPOSE_TILE_BLOCK_ROWS (16)

/**
 * A structure describing a face of a multi-dimensional (in our case 3D)
 * rectanguluar prism It is effectively a targetted reimplementation of gslice
 */
typedef struct {
  /** Starting offset in uni-dimensional buffer */
  int start;
  /** The size of size and stride buffers (number of tree levels) */
  int count;
  /** The number of samples at each tree level */
  int *size;
  /** The distance between samples at each tree level */
  int *stride;
} meta_face;
/**
 * Pack the provided face specification information into a new struct, copying
 * the arrays This is largely for compatibility so an existing slice can be used
 * by MetaMorph calls
 * \param s Position of the first element
 * \param c number of elements in the size and stride arrays
 * \param si an array that defines the number of elements in each dimension
 * \param st an array that defines the number of positions between successive
 * elements in each dimension
 * \return A dynamically-allocated face object with the corresponding data
 */
meta_face *meta_get_face(int s, int c, int *si, int *st);
/**
 * Release a face and its internal size and stride arrays
 * \param face the object to release
 */
void meta_free_face(meta_face *face);
meta_face *make_slab_from_3d(int face, int ni, int nj, int nk, int thickness);

/// \todo move all these typedefs, the generic types need to be runtime controlled
/// \todo should generic "accelerator" types be typedefs, unions, ...?
/// \todo implement generic accelerators types so an accelerator model can be chosen at runtime
/** Optional typedef to ensure internal types have predictable size */
typedef double meta_double;
/** Optional typedef to ensure internal types have predictable size */
typedef float meta_float;
/** Optional typedef to ensure internal types have predictable size */
typedef int meta_int;
/** Optional typedef to ensure internal types have predictable size */
typedef long meta_long;
/** Optional typedef to ensure internal types have predictable size */
typedef short meta_short;
/** Optional typedef to ensure internal types have predictable size */
typedef char meta_char;
/** Optional typedef to ensure internal types have predictable size */
typedef unsigned char meta_uchar;
/** Optional typedef to ensure internal types have predictable size */
typedef unsigned short meta_ushort;
/** Optional typedef to ensure internal types have predictable size */
typedef unsigned int meta_uint;
/** Optional typedef to ensure internal types have predictable size */
typedef unsigned long meta_ulong;
/** Define a standard type for specifying parameters of three or less dimensions
 */
typedef size_t meta_dim3[3];

#ifdef UNIMPLEMENTED
/**
 * Originally, all MetaMorph operations were supposed to be thread-safe via
 * hazard pointers This struct was the beginning of implementing that, but this
 * aspirational goal was shelved
 */
typedef struct HPRecType {
  void *HP[2];
  struct HPRecType *next;
  void *rlist; /// \todo implement the rlist type
  int rcount;
  char active;
} HPRecType;
#endif
// Shared HP variables

/**
 * A simple abstract type to store the type of event and a payload pointer
 * containing the actual backend-specific value
 * \todo FIXME meta_events are currently allocated and freed by the client, but
 * assumed to persist throughout. It would be safer to have our own copy the
 * user cannot tamper with
 */
typedef struct meta_event {
  /** The backend mode active when the event was created, how to interpret the
   * event_pl */
  meta_preferred_mode mode;
  /** The event's "payload", dynamically allocated to the backend-specific type
   * when the event is created */
  void *event_pl;
} meta_event;

/** Global destructor */
void meta_finalize();
/** Global constructor */
void meta_init();

/**
 * Lookup all modules with a matching implementation signature
 * \param retRecords a pointer to a storage buffer for matched record pointers
 * \param szRetRecords the size of the retRecords buffer
 * \param signature the signature to match on
 * \param matchAny if true, will match if the record and signature have any
 * matching flags (binary OR), otherwise will only match exactly (binary AND)
 * \return the number of matched modules, regardless of how many were actually
 * storable in retRecords
 */
int lookup_implementing_modules(meta_module_record **retRecords,
                                size_t szRetRecords,
                                meta_module_implements_backend signature,
                                meta_bool matchAny);
/**
 * Reinitialize all modules with a given implements_backend mask
 * It will reinitialize them using a binary AND of the mask and each module's
 * reported capability This function is typically called internally when the
 * MetaMorph internal state changes (For example, when the OpenCL backend
 * switches to a new device, it will trigger this with meta_implements_opencl so
 * that the module can re-create cl_programs and cl_kernels for the new device.)
 * \param module_type The mask of supported capabilitys
 * \return -1 if there are no known modules, 0 otherwise
 */
meta_err meta_reinitialize_modules(meta_module_implements_backend module_type);
/**
 * Allocate a MetaMorph buffer on the currently-active backend and device
 * In most cases the returned pointer should not be tampered with on the host as
 * it maps to a backend type
 * \param ptr The address to return the new buffer's handle (not pointer) in
 * \param size The size of the buffer to allocate
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_alloc(void **ptr, size_t size);
/**
 * Release a MetaMorph-allocated backend buffer that resides on the
 * currently-active backend
 * \param ptr The MetaMorph buffer handle returned by meta_alloc
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_free(void *ptr);
/**
 * Checks that grid_size is a valid multiple of block_size, and that
 * both obey the bounds specified for the device/execution model currently in
 * use. For now CUDA and OpenMP are NOOP For now it does not directly modify the
 * work size at all nor does it return the ideal worksize in any way it just
 * produces a STDERR comment stating which variable is out of bounds what the
 * bound is, and what the variable is currently.
 * \param grid_size desired number of thread blocks in each dimension
 * \param block_size desired thread dimensions within each thread block
 * \return 0 on success or NOOP, -1 if there is a problem with the configuration
 */
meta_err meta_validate_worksize(meta_dim3 *grid_size, meta_dim3 *block_size);
/**
 * Flush does exactly what it sounds like - forces any outstanding work to be
 * finished before returning For now it handles flushing async kernels, and
 * finishing outstanding transfers from the MPI plugin It could easily handle
 * timer dumping as well, but that might not be ideal
 * \return 0 on success, -1 or the backend-specific error code otherwise
 */
meta_err meta_flush();

/**
 * Simple initializer for an event that sets the mode to whatever we are
 * currently using, creates a heap-allocated payload corresponding to that
 * backend, and uses the backend's initializer to prepare the payload
 * \param event An address in which to return the dynamically-allocated new
 * event
 * \return 0 on success, -1 or a backend-specific error code otherwise
 */
meta_err meta_init_event(meta_event *event);
/**
 * Simple destructor that frees the backend-agnostic void* event payload
 * \param event The event to release the payload from
 * \return 0 on success, -1 or a backend-specific error code on failure
 */
meta_err meta_destroy_event(meta_event event);

/** Forward declaration of the structure used to contain backend-agnostic
 * MetaMorph callback objects */
struct meta_callback;
/**
 * MetaMorph needs a backend-agnostic way of handling callback functions to
 * broadly support asynchronous execution This structure provides a way of
 * recording both the function pointer and data payload, and any additional
 * information added by the backend's callback implementation
 * \todo FIXME meta_callbacks are currently allocated and freed by the client,
 * but assumed to persist throughout. It would be safer to have our own copy the
 * user cannot tamper with
 */
typedef struct meta_callback {
  /** The callback function should only take this object as an argument, and has
   * the responsibily to unpack it */
  void (*callback_func)(struct meta_callback *);
  /** The execution mode when the callback was registered */
  meta_preferred_mode callback_mode;
  /** The actual data payload to pass to whetever the final function is */
  void *data_payload;
  /** A pointer to a backend-specific struct with any additional information the
   * backend provides to the callback (event, stream, status, etc.) */
  void *backend_status;
} meta_callback;

/**
 * Wrapper for writes to the device
 * \param dst The destination buffer, a MetaMorph handle allocated on the
 * currently-running backend and device
 * \param src The source buffer, a host memory region
 * \param size The number of bytes to copy from the host to the device
 * \param async whether the write should be asynchronous or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the write back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_copy_h2d(void *dst, void *src, size_t size, meta_bool async,
                    meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for reads from the device
 * \param dst The destination buffer, a host memory region
 * \param src The source buffer, a MetaMorph handle allocated on the
 * currently-running backend and device
 * \param size The number of bytes to copy from the device to the host
 * \param async whether the read should be asynchronous or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_copy_d2h(void *dst, void *src, size_t size, meta_bool async,
                    meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for on-device copies of MetaMorph-allocated buffers
 * \param dst The destination buffer, a MetaMorph handle allocated on the
 * currently-running backend and device
 * \param src The source buffer, a MetaMorph handle allocated on the
 * currently-running backend and device
 * \param size The number of bytes to copy
 * \param async whether the copy should be asynchronous or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_copy_d2d(void *dst, void *src, size_t size, meta_bool async,
                    meta_callback *call, meta_event *ret_event);
/**
 * Wrapper to transpose a 2D array
 * \param grid_size The number of blocks in each dimension (NOT the global work
 * size)
 * \param block_size The size of a block in each dimension
 * \param indata The input untransposed 2D array
 * \param outdata The output transposed 2D array
 * \param arr_dim_xy the X and Y dimensions of indata, Z is ignored
 * \param tran_dim_xy the X and Y dimensions of outdata, Z is ignored
 * \param type The data type to invoke the appropriate kernel for
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 * \bug OpenCL return codes should not be binary OR'd, rather separately checked
 * and the last error returned
 */
meta_err meta_transpose_face(meta_dim3 *grid_size, meta_dim3 *block_size, void *indata,
                          void *outdata, meta_dim3 *arr_dim_xy,
                          meta_dim3 *tran_dim_xy, meta_type_id type, meta_bool async,
                          meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for the face packing kernel
 * \param grid_size The number of workgroups to run in the X dimension, Y and Z are ignored (NOT the global worksize)
 * \param block_size The size of a workgroup in the X dimension, Y and Z are ignored
 * \param packed_buf The output packed array
 * \param buf The input full array
 * \param face The face/slab to extract, returned from make_slab_from_3d
 * \param type The type of data to run the kernel on
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param ret_event_k1 Address of a meta_event with initialized backend payload in which to return the event for the kernel execution
 * \param ret_event_c1 Address of a meta_event with initialized backend payload in which to return the event for the write of face->size
 * \param ret_event_c2 Address of a meta_event with initialized backend payload in which to return the event for the write of face->stride
 * \param ret_event_c3 Address of a meta_event with initialized backend payload in which to return the event for the write of precomputed interior dimension sizes
 * \param call A callback to run when the kernel finishes, or NULL if none
 * \return -1 if the current backend's implementation wasn't loaded properly, otherwise the results of the backend implementation, which is directly castable to that backend's internal error type
 * \warning Implemented as a 1D kernel, Y and Z grid/block parameters will be ignored
/// \todo fix frame->size to reflect face size
 */
meta_err meta_pack_face(meta_dim3 *grid_size, meta_dim3 *block_size, void *packed_buf,
                     void *buf, meta_face *face, meta_type_id type,
                     meta_bool async, meta_callback *call,
                     meta_event *ret_event_k1, meta_event *ret_event_c1,
                     meta_event *ret_event_c2, meta_event *ret_event_c3);
/**
 * Wrapper for the face unpacking kernel
 * \param grid_size The number of workgroups to run in the X dimension, Y and Z are ignored (NOT the global worksize)
 * \param block_size The size of a workgroup in the X dimension, Y and Z are ignored
 * \param packed_buf The input packed array
 * \param buf The output full array
 * \param face The face/slab to extract, returned from make_slab_from_3d
 * \param type The type of data to run the kernel on
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param ret_event_k1 Address of a meta_event with initialized backend payload in which to return the event for the kernel execution
 * \param ret_event_c1 Address of a meta_event with initialized backend payload in which to return the event for the write of face->size
 * \param ret_event_c2 Address of a meta_event with initialized backend payload in which to return the event for the write of face->stride
 * \param ret_event_c3 Address of a meta_event with initialized backend payload in which to return the event for the write of precomputed interior dimension sizes
 * \param call A callback to run when the kernel finishes, or NULL if none
 * \return -1 if the current backend's implementation wasn't loaded properly, otherwise the results of the backend implementation, which is directly castable to that backend's internal error type
 * \warning Implemented as a 1D kernel, Y and Z grid/block parameters will be ignored
/// \todo fix frame->size to reflect face size
 */
meta_err meta_unpack_face(meta_dim3 *grid_size, meta_dim3 *block_size, void *packed_buf,
                       void *buf, meta_face *face, meta_type_id type,
                       meta_bool async, meta_callback *call,
                       meta_event *ret_event_k1, meta_event *ret_event_c1,
                       meta_event *ret_event_c2, meta_event *ret_event_c3);
/**
 * Dot-product of identically-shaped subregions of two identically-shaped 3D
 * arrays this kernel works for 3D data only.
 * \param grid_size The number of blocks in each global dimension (not global
 * work-items)
 * \param block_size The dimensions of each block
 * \param data1 first input array (a MetaMorph buffer handle allocated on the
 * currently-active backend and device)
 * \param data2 second input array (a MetaMorph buffer handle allocated on the
 * currently-active backend and device)
 * \param array_size X, Y, and Z dimension sizes of the input arrays (must
 * match)
 * \param array_start X, Y, and Z dimension start indicies for computing on a
 * subregion, to allow for a halo region
 * \param array_end X, Y, and Z dimension end indicies for computing on a
 * subregion, to allow for a halo region
 * \param reduction_var the final dotproduct output (scalar) across all
 * workgroups, assumed to be initialized before the kernel (a MetaMorph buffer
 * handle allocated on the currently-active backend and device)
 * \param type The supported MetaMorph data type that data1, data2, and
 * reduction_var contain (Currently: meta_db, meta_fl, meta_ul, meta_in, meta_ui)
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_dotProd(meta_dim3 *grid_size, meta_dim3 *block_size, void *data1,
                   void *data2, meta_dim3 *array_size, meta_dim3 *array_start,
                   meta_dim3 *array_end, void *reduction_var, meta_type_id type,
                   meta_bool async, meta_callback *call, meta_event *ret_event);
/**
 * Reduction sum of identically-shaped subregions of two identically-shaped 3D
 * arrays this kernel works for 3D data only.
 * \param grid_size The number of blocks in each global dimension (not global
 * work-items)
 * \param block_size The dimensions of each block
 * \param data input array (a MetaMorph buffer handle allocated on the
 * currently-active backend and device)
 * \param array_size X, Y, and Z dimension sizes of the input arrays (must
 * match)
 * \param array_start X, Y, and Z dimension start indicies for computing on a
 * subregion, to allow for a halo region
 * \param array_end X, Y, and Z dimension end indicies for computing on a
 * subregion, to allow for a halo region
 * \param reduction_var the final dotproduct output (scalar) across all
 * workgroups, assumed to be initialized before the kernel (a MetaMorph buffer
 * handle allocated on the currently-active backend and device)
 * \param type The supported MetaMorph data type that data and reduction_var
 * contain (Currently: meta_db, meta_fl, meta_ul, meta_in, meta_ui)
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_reduce(meta_dim3 *grid_size, meta_dim3 *block_size, void *data,
                  meta_dim3 *array_size, meta_dim3 *array_start, meta_dim3 *array_end,
                  void *reduction_var, meta_type_id type, meta_bool async,
                  meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for the 3D 7-point stencil averaging kernel
 * \param grid_size The number of workgroups to run in each dimension (NOT the
 * global worksize)
 * \param block_size The size of a workgroup in each dimension
 * \param indata The array of input read values
 * \param outdata The array of output write values
 * \param array_size The size of the input and output arrays
 * \param array_start The start index for writing in each dimension (for
 * avoiding writes to halo cells)
 * \warning Assumes at least a one-thick halo region i.e. (arr_start[dim]-1 >=
 * 0)
 * \param array_end The end index for writing in each dimension (for avoiding
 * writes to halo cells)
 * \warning Assumes at least a one-thick halo region i.e. (arr_end[dim]+1 <=
 * array_size[dim]-1)
 * \param type The type of data to run the kernel on
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 */
meta_err meta_stencil_3d7p(meta_dim3 *grid_size, meta_dim3 *block_size, void *indata,
                        void *outdata, meta_dim3 *array_size, meta_dim3 *array_start,
                        meta_dim3 *array_end, meta_type_id type, meta_bool async,
                        meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for the SPMV kernel for CSR sparse matrix format
 * \param grid_size The number of workgroups to run in the X dimension, Y and Z
 * are ignored (NOT the global worksize)
 * \param block_size The size of a workgroup in each dimension
 * \param global_size The number of rows in the A matrix, one computed per
 * work-item
 * \param csr_ap The row start and end index array
 * \param csr_aj The column index array
 * \param csr_ax The cell value array
 * \param x_loc The input vector to multiply A by
 * \param y_loc The output vector to sum into
 * \param type The supported MetaMorph data type that csr_ax, x_loc, and y_loc
 * contain (Currently: meta_db, meta_fl, meta_ul, meta_in, meta_ui)
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 * \warning Y and Z dimensions are ignored
 * \todo FIXME Only exists for OpenCL backend
 */
meta_err meta_csr(meta_dim3 *grid_size, meta_dim3 *block_size, size_t global_size,
               void *csr_ap, void *csr_aj, void *csr_ax, void *x_loc,
               void *y_loc, meta_type_id type, meta_bool async,
               meta_callback *call, meta_event *ret_event);
/**
 * Wrapper for the cyclic redundancy check task kernel
 * \param grid_size The number of blocks to run in the X dimension, Y and Z are
 * ignored (NOT the global worksize)
 * \param block_size The size of a block in each dimension
 * \param dev_input The input buffer to perform the redundancy check on
 * \param page_size The length in bytes of each page
 * \param num_words TODO
 * \param numpages TODO
 * \param dev_output The result
 * \param type The supported MetaMorph data type that dev_input contains
 * (Currently: meta_db, meta_fl, meta_ul, meta_in, meta_ui)
 * \param async Whether the kernel should be run asynchronously or blocking
 * \param call A callback to run when the transfer finishes, or NULL if none
 * \param ret_event The address of a meta_event with initialized backend payload
 * in which to copy the bacckend event corresponding to the read back to
 * \return -1 if the current backend's implementation wasn't loaded properly,
 * otherwise the results of the backend implementation, which is directly
 * castable to that backend's internal error type
 * \warning all supported types are interpreted as binary data via the unsigned
 * int kernel
 * \todo FIXME Only exists for OpenCL backend
 */
meta_err meta_crc(meta_dim3 *grid_size, meta_dim3 *block_size, void *dev_input,
               int page_size, int num_words, int numpages, void *dev_output,
               meta_type_id type, meta_bool async, meta_callback *call,
               meta_event *ret_event);

#ifdef __cplusplus
}
#endif

#endif // METAMORPH_H
